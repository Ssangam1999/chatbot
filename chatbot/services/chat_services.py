import os
from typing import Dict, Any

from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaLLM, OllamaEmbeddings

from chatbot.configs.embedding_model_config import embedding_model
from chatbot.configs.ollama_model_config import ollama_model


class ChatEngine:
    """
    An object-oriented interface for performing RAG-based question answering.
    It handles loading the vectorstore, retrieving relevant documents, and generating answers.
    """

    def __init__(self, vectorstore_path: str = None):
        """
        Initializes the ChatEngine by loading a FAISS vectorstore and the LLM.

        :param vectorstore_path: Path to the saved FAISS vector index directory.
        """
        if vectorstore_path is None:
            current_path = os.path.dirname(os.path.abspath(__file__))
            vectorstore_path = os.path.join(current_path, "..", "..", "faiss_index")

        self.embedding_model = OllamaEmbeddings(model=embedding_model)
        self.vectorstore = FAISS.load_local(
            vectorstore_path,
            self.embedding_model,
            allow_dangerous_deserialization=True
        )
        self.llm = OllamaLLM(model=ollama_model)

    def retrieve_documents(self, query: str, k: int = 4):
        """
        Retrieves top-k relevant documents for the given query using vector similarity.

        :param query: The user's query string.
        :param k: Number of top documents to retrieve.
        :return: List of relevant documents.
        """
        return self.vectorstore.similarity_search(query, k=k)

    def generate_answer(self, query: str):
        """
        Retrieves relevant context and generates an answer from the LLM.

        :param query: The user's query string.
        :return: Answer string generated by the LLM.
        """
        docs = self.retrieve_documents(query)
        context = "\n\n".join(doc.page_content for doc in docs)
        prompt = f"Context:\n{context}\n\nQuestion: {query}"
        return self.llm.invoke(prompt)

    def run(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Integrates document retrieval and response generation for use in stateful workflows.

        :param state: A dictionary containing a "query" key.
        :return: Updated state with "docs" and "answer".
        """
        query = state.get("query", "")
        docs = self.retrieve_documents(query)
        answer = self.generate_answer(query)
        return {**state, "docs": docs, "answer": answer}


